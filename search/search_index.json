{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"<p>The webpage was created using github Pages and Mkdocs (Mkdocs Material Theme), refer to the full documentation of the theme for more information. </p> <p>In this documentation you will find the basic and most important tips to create and edit the documentation web pages alongside some additional features, more information here.</p>"},{"location":"documentation/basic/","title":"Basic Information","text":""},{"location":"documentation/basic/#creating-file","title":"Creating File","text":"<p>Create an Markdown file (.md) in the right location (check the file structure below), then create a new branch for your commit and start a pull request. Once it is approved your page will be added to the website</p>"},{"location":"documentation/basic/#editing-file","title":"Editing File","text":"<p>To edit a document, from the website page directly click on the \"Edit this page\" button on the top right of your document/page you will be automatically redirected into the github repository in order to change the md file. Or just search for the file in the repository manually. Once you finish editing make a pull request for your changes and wait for the merge approve to see the changes.</p>"},{"location":"documentation/basic/#file-structure","title":"File Structure","text":"<p>Follow this structure and the top and side bars will be automatically updated. Here is a example of the current file structure, keep the same pattern.</p> <p><pre><code>+-- ..\n|-- (files)\n|\n|-- docs\n|   |-- home.md \n|   |-- documentation\n|   |   |-- basic.md \n|   |   |-- features.md\n|   |\n|   |-- pyspark\n|   |   |-- python.md\n|   |   |-- spark.md\n|   |   |-- examples\n|   |   |   |-- first.md\n|   |\n|   |-- (other md files)\n|   +-- ..\n|\n|-- (files)\n+-- ..\n</code></pre> So consider it as a normal folder and files. The inital folder will be added as a top navigation item and everything inside this folder will be in the side navigation bar, where you can also create another folder and you will have dropdown in the sidebar.</p>"},{"location":"documentation/basic/#navigation","title":"Navigation","text":"<p>The nav configuration setting in your mkdocs.yml file defines which pages are included in the global site navigation menu as well as the structure of that menu. If not provided, the navigation will be automatically created by discovering all the Markdown files in the documentation directory. An automatically created navigation configuration will always be sorted alphanumerically by file name (except that index files will always be listed first within a sub-section). You will need to manually define your navigation configuration if you would like your navigation menu sorted differently. To change the file order, edit the <code>mkdocs.yml</code> file by adding this code <pre><code>nav:\n    - Home: 'index.md'\n    - 'User Guide':\n        - 'Writing your docs': 'writing-your-docs.md'\n        - 'Styling your docs': 'styling-your-docs.md'\n    - About:\n        - 'License': 'license.md'\n        - 'Release Notes': 'release-notes.md'\n</code></pre></p>"},{"location":"documentation/features/","title":"Features","text":"<p>Please refer to Mkdocs and Mkdocs Material Theme for the full documentation and features, since everything is customizable and here you can only find the most used features</p>"},{"location":"documentation/features/#hiding-siderbars","title":"Hiding Siderbars","text":"<p>Add the following code to the top of your Markdown file to hide the table of content(toc) and/or the sidebar(navigation) <pre><code>---\nhide:\n  - navigation\n  - toc\n---\n\n# Document title\n...\n</code></pre></p>"},{"location":"documentation/features/#exclude-search","title":"Exclude Search","text":"<p>Pages can be excluded from search. Add the following lines at the top of a Markdown file: <pre><code>---\nsearch:\n  exclude: true\n---\n\n# Document title\n...\n</code></pre></p>"},{"location":"documentation/features/#tip","title":"Tip","text":"<pre><code>!!! tip \"Title\"\n\n    Text...\n</code></pre> <p>Title</p> <p>Text...</p>"},{"location":"documentation/features/#warning","title":"Warning","text":"<pre><code>!!! warning \"Title\"\n\n    Text...\n</code></pre> <p>Title</p> <p>Text...</p>"},{"location":"documentation/features/#success","title":"Success","text":"<pre><code>!!! success \"Title\"\n\n    Text...\n</code></pre> <p>Title</p> <p>Text...</p>"},{"location":"documentation/features/#failure","title":"Failure","text":"<pre><code>!!! failure \"Title\"\n\n    Text...\n</code></pre> <p>Title</p> <p>Text...</p>"},{"location":"documentation/features/#danger","title":"Danger","text":"<pre><code>!!! danger \"Title\"\n\n    Text...\n</code></pre> <p>Title</p> <p>Text...</p>"},{"location":"documentation/features/#example","title":"Example","text":"<pre><code>!!! example \"Title\"\n\n    Text...\n</code></pre> <p>Title</p> <p>Text...</p>"},{"location":"documentation/features/#quote","title":"Quote","text":"<pre><code>!!! quote \"Title\"\n\n    Text...\n</code></pre> <p>Title</p> <p>Text...</p>"},{"location":"documentation/features/#note","title":"Note","text":"<pre><code>!!! note \"Title\"\n\n    Text...\n</code></pre> <p>Title</p> <p>Text...</p>"},{"location":"documentation/features/#collapsible-blocks","title":"Collapsible blocks","text":"<p>Can be used for note, tip, warning, success</p> <pre><code>??? note\n\n    Test...\n</code></pre> Note <p>Test...</p>"},{"location":"documentation/features/#highlights-code-line","title":"Highlights Code Line","text":"Code block with highlighted lines<pre><code>``` py hl_lines=\"2 3\"\ndef bubble_sort(items):\n    for i in range(len(items)):\n        for j in range(len(items) - 1 - i):\n            if items[j] &gt; items[j + 1]:\n                items[j], items[j + 1] = items[j + 1], items[j]\n```\n</code></pre> <pre><code>def bubble_sort(items):\nfor i in range(len(items)):\nfor j in range(len(items) - 1 - i):\nif items[j] &gt; items[j + 1]:\n                items[j], items[j + 1] = items[j + 1], items[j]\n</code></pre>"},{"location":"documentation/features/#grouping-content","title":"Grouping Content","text":""},{"location":"documentation/features/#list-example","title":"List Example","text":"<pre><code>=== \"Unordered list\"\n\n    * Sed sagittis eleifend rutrum\n    * Donec vitae suscipit est\n    * Nulla tempor lobortis orci\n\n=== \"Ordered list\"\n\n    1. Sed sagittis eleifend rutrum\n    2. Donec vitae suscipit est\n    3. Nulla tempor lobortis orci\n</code></pre> Unordered listOrdered list <ul> <li>Sed sagittis eleifend rutrum</li> <li>Donec vitae suscipit est</li> <li>Nulla tempor lobortis orci</li> </ul> <ol> <li>Sed sagittis eleifend rutrum</li> <li>Donec vitae suscipit est</li> <li>Nulla tempor lobortis orci</li> </ol>"},{"location":"documentation/features/#coding-example","title":"Coding Example","text":"<pre><code>=== \"C\"\n\n    ``` c\n    #include &lt;stdio.h&gt;\n\n    int main(void) {\n      printf(\"Hello world!\\n\");\n      return 0;\n    }\n    ```\n\n=== \"C++\"\n\n    ``` c++\n    #include &lt;iostream&gt;\n\n    int main(void) {\n      std::cout &lt;&lt; \"Hello world!\" &lt;&lt; std::endl;\n      return 0;\n    }\n    ```\n</code></pre> CC++ <pre><code>#include &lt;stdio.h&gt;\n\nint main(void) {\nprintf(\"Hello world!\\n\");\nreturn 0;\n}\n</code></pre> <pre><code>#include &lt;iostream&gt;\n\nint main(void) {\nstd::cout &lt;&lt; \"Hello world!\" &lt;&lt; std::endl;\nreturn 0;\n}\n</code></pre>"},{"location":"documentation/features/#column-alignment-in-table","title":"Column alignment in table","text":"<p>If you want to align a specific column to the <code>left</code>, <code>center</code> or <code>right</code>, you can use the [regular Markdown syntax] placing <code>:</code> characters at the beginning and/or end of the divider.</p> LeftCenterRight Data table, columns aligned to left<pre><code>| Method      | Description                          |\n| :---------- | :----------------------------------- |\n| `GET`       | :material-check:     Fetch resource  |\n| `PUT`       | :material-check-all: Update resource |\n| `DELETE`    | :material-close:     Delete resource |\n</code></pre> Method Description <code>GET</code> :material-check:     Fetch resource <code>PUT</code> :material-check-all: Update resource <code>DELETE</code> :material-close:     Delete resource Data table, columns centered<pre><code>| Method      | Description                          |\n| :---------: | :----------------------------------: |\n| `GET`       | :material-check:     Fetch resource  |\n| `PUT`       | :material-check-all: Update resource |\n| `DELETE`    | :material-close:     Delete resource |\n</code></pre> Method Description <code>GET</code> :material-check:     Fetch resource <code>PUT</code> :material-check-all: Update resource <code>DELETE</code> :material-close:     Delete resource Data table, columns aligned to right<pre><code>| Method      | Description                          |\n| ----------: | -----------------------------------: |\n| `GET`       | :material-check:     Fetch resource  |\n| `PUT`       | :material-check-all: Update resource |\n| `DELETE`    | :material-close:     Delete resource |\n</code></pre> Method Description <code>GET</code> :material-check:     Fetch resource <code>PUT</code> :material-check-all: Update resource <code>DELETE</code> :material-close:     Delete resource"},{"location":"documentation/features/#text-formating","title":"Text Formating","text":"<p><pre><code>Text can be deleted and replacement text added. This can also be\ncombined into onea single operation. Highlighting is also\npossible and comments can be added inline.\n\n\n\nFormatting can also be applied to blocks by putting the opening and closing\ntags on separate lines and adding new lines between the tags and the content.\n\n\n</code></pre> Text can be deleted and replacement text added. This can also be combined into onea single operation. Highlighting is also possible and comments can be added inline.</p> <p>Formatting can also be applied to blocks by putting the opening and closing tags on separate lines and adding new lines between the tags and the content.</p>"},{"location":"documentation/features/#diagram","title":"Diagram","text":"<p>There are several diagrams but here is one example, to check all the others check it here</p> Class diagram<pre><code>``` mermaid\nclassDiagram\n  Person &lt;|-- Student\n  Person &lt;|-- Professor\n  Person : +String name\n  Person : +String phoneNumber\n  Person : +String emailAddress\n  Person: +purchaseParkingPass()\n  Address \"1\" &lt;-- \"0..1\" Person:lives at\n  class Student{\n    +int studentNumber\n    +int averageMark\n    +isEligibleToEnrol()\n    +getSeminarsTaken()\n  }\n  class Professor{\n    +int salary\n  }\n  class Address{\n    +String street\n    +String city\n    +String state\n    +int postalCode\n    +String country\n    -validate()\n    +outputAsLabel()  \n  }\n```\n</code></pre> <pre><code>classDiagram\n  Person &lt;|-- Student\n  Person &lt;|-- Professor\n  Person : +String name\n  Person : +String phoneNumber\n  Person : +String emailAddress\n  Person: +purchaseParkingPass()\n  Address \"1\" &lt;-- \"0..1\" Person:lives at\n  class Student{\n    +int studentNumber\n    +int averageMark\n    +isEligibleToEnrol()\n    +getSeminarsTaken()\n  }\n  class Professor{\n    +int salary\n  }\n  class Address{\n    +String street\n    +String city\n    +String state\n    +int postalCode\n    +String country\n    -validate()\n    +outputAsLabel()  \n  }</code></pre>"},{"location":"pyspark/python/","title":"PySpark Style Guide (Preview)","text":""},{"location":"pyspark/python/#introduction","title":"Introduction","text":"<p>Idea of this code stype guide is improve consistency of our pipeline development in Foundry. Help teams write readable and maintainable programs using Apache PySpark.</p> <p>This opinionated guide to PySpark code style presents common situations we've encountered and the associated best practices based on the most frequent recurring topics across PySpark repos.</p> <p>Beyond PySpark specifics, the general practices of clean code are important in PySpark repositories- the Google PyGuide is a strong starting point for learning more about these practices.</p> <p>This pyspark code stype guide is based on palantir/pyspark-style-guide.</p>"},{"location":"pyspark/python/#general","title":"General","text":""},{"location":"pyspark/python/#easy-to-read-consistent-explicit","title":"Easy to read, consistent, explicit","text":""},{"location":"pyspark/python/#imports","title":"Imports","text":"<p>Import pyspark functions, types, and window narrowly and with consistent aliases. <pre><code># bad\nfrom pyspark import *\n\n# bad\nfrom pyspark.sql.functions import *\n\n# good\nfrom pyspark.sql import functions as F\nfrom pyspark.sql import types as T\nfrom pyspark.sql import window as W\nfrom pyspark.sql import SparkSession, DataFrame\n</code></pre> This prevents name collisions, as many PySpark functions have common names. This will make our code more consistent.</p>"},{"location":"pyspark/python/#use-descriptive-names-for-dataframes","title":"Use descriptive names for dataframes","text":"<p><pre><code># bad\ndef get_large_downloads(df):\n    return df.where(F.col(\"size\") &gt; 100)\n\n\n# good\ndef get_large_downloads(downloads):\n    return downloads.where(F.col(\"size\") &gt; 100)\n</code></pre> Good naming is common practice for normal Python functions, Finding appropriate names for dataframes makes code easier to understand quickly.</p>"},{"location":"pyspark/python/#use-column-name-strings-to-access-columns","title":"Use column name strings to access columns","text":"<p><pre><code># bad\ndf.col_name\n\n\n# good\nF.col(\"col_name\")\n</code></pre> Using the dot notation presents several problems. It can lead to name collisions if a column shares a name with a dataframe method. It requires the version of the dataframe with the column you want to access to be bound to a variable, but it may not be. It also doesn't work if the column name contains certain non-letter characters. The most obvious exception to this are joins where the joining column has a different name in the two tables. <pre><code># OK\ndownloads.join(\n  users, \n  downloads.user_id == user.id,\n  how='inner',\n)\n</code></pre></p>"},{"location":"pyspark/python/#when-a-function-accepts-a-column-or-column-name-use-the-column-name-option","title":"When a function accepts a column or column name, use the column name option","text":"<p><pre><code># bad\nF.collect_set(F.col(\"client_ip\"))\n\n\n# good\nF.collect_set(\"client_ip\")\n</code></pre> Expressions are easier to read without the extra noise of <code>F.col()</code>.</p>"},{"location":"pyspark/python/#when-the-output-of-a-function-is-stored-as-a-column-give-the-column-a-concise-name","title":"When the output of a function is stored as a column, give the column a concise name","text":"<p><pre><code># bad\nresult = logs.groupby(\"user_id\").agg(\n    F.count(\"operation\"),\n)\nresult.printSchema()\n\n# root\n#  |-- user_id: string\n#  |-- count(operation): long\n\n\n# good\nresult = logs.groupby(\"user_id\").agg(\n    F.count(\"operation\").alias(\"operation_count\"),\n)\nresult.printSchema()\n\n# root\n# |-- user_id: string\n# |-- operation_count: long\n</code></pre> The default column names are usually awkward, probably defy the naming style of other columns, and can get long.</p>"},{"location":"pyspark/python/#empty-columns","title":"Empty columns","text":"<p>If you need to add an empty column to satisfy a schema, always use <code>F.lit(None)</code> for populating that column. Never use an empty string or some other string signalling an empty value (such as <code>NA</code>).</p> <p>Beyond being semantically correct, one practical reason for using <code>F.lit(None)</code> is preserving the ability to use utilities like <code>isNull</code>, instead of having to verify empty strings, nulls, and <code>'NA'</code>, etc.</p> <pre><code># bad\ndf = df.withColumn('foo', F.lit(''))\n\n# bad\ndf = df.withColumn('foo', F.lit('NA'))\n\n\n# good\ndf = df.withColumn('foo', F.lit(None))\n</code></pre>"},{"location":"pyspark/python/#using-comments","title":"Using comments","text":"<p>While comments can provide useful insight into code, it is often more valuable to refactor the code to improve its readability. The code should be readable by itself. If you are using comments to explain the logic step by step, you should refactor it.</p> <pre><code># bad\n\n# Cast the timestamp columns\ncols = ['start_date', 'delivery_date']\nfor c in cols:\n    df = df.withColumn(c, F.from_unixtime(F.col(c) / 1000).cast(TimestampType()))\n</code></pre> <p>In the example above, we can see that those columns are getting cast to Timestamp. The comment doesn't add much value. Moreover, a more verbose comment might still be unhelpful if it only provides information that already exists in the code. For example:</p> <pre><code># bad\n\n# Go through each column, divide by 1000 because millis and cast to timestamp\ncols = ['start_date', 'delivery_date']\nfor c in cols:\n    df = df.withColumn(c, F.from_unixtime(F.col(c) / 1000).cast(TimestampType()))\n</code></pre> <p>Instead of leaving comments that only describe the logic you wrote, aim to leave comments that give context, that explain the \"why\" of decisions you made when writing the code. This is particularly important for PySpark, since the reader can understand your code, but often doesn't have context on the data that feeds into your PySpark transform. Small pieces of logic might have involved hours of digging through data to understand the correct behavior, in which case comments explaining the rationale are especially valuable.</p> <pre><code># good\n\n# The consumer of this dataset expects a timestamp instead of a date, and we need\n# to adjust the time by 1000 because the original datasource is storing these as millis\n# even though the documentation says it's actually a date.\ncols = ['start_date', 'delivery_date']\nfor c in cols:\n    df = df.withColumn(c, F.from_unixtime(F.col(c) / 1000).cast(TimestampType()))\n</code></pre>"},{"location":"pyspark/python/#logical-operations","title":"Logical operations","text":""},{"location":"pyspark/python/#refactor-complex-logical-operations","title":"Refactor complex logical operations","text":"<p>Logical operations, which often reside inside <code>.filter()</code> or <code>F.when()</code>, need to be readable. We apply the same rule as with chaining functions, keeping logic expressions inside the same code block to three (3) expressions at most. If they grow longer, it is often a sign that the code can be simplified or extracted out. Extracting out complex logical operations into variables makes the code easier to read and reason about, which also reduces bugs.</p> <pre><code># bad\nF.when( (F.col('prod_status') == 'Delivered') | (((F.datediff('deliveryDate_actual', 'current_date') &lt; 0) &amp; ((F.col('currentRegistration') != '') | ((F.datediff('deliveryDate_actual', 'current_date') &lt; 0) &amp; ((F.col('originalOperator') != '') | (F.col('currentOperator') != '')))))), 'In Service')\n</code></pre> <p>The code above can be simplified in different ways. To start, focus on grouping the logic steps in a few named variables. PySpark requires that expressions are wrapped with parentheses. This, mixed with actual parenthesis to group logical operations, can hurt readability. For example the code above has a redundant <code>(F.datediff(df.deliveryDate_actual, df.current_date) &lt; 0)</code> that the original author didn't notice because it's very hard to spot.</p> <pre><code># better\nhas_operator = ((F.col('originalOperator') != '') | (F.col('currentOperator') != ''))\ndelivery_date_passed = (F.datediff('deliveryDate_actual', 'current_date') &lt; 0)\nhas_registration = (F.col('currentRegistration').rlike('.+'))\nis_delivered = (F.col('prod_status') == 'Delivered')\n\nF.when(is_delivered | (delivery_date_passed &amp; (has_registration | has_operator)), 'In Service')\n</code></pre> <p>The above example drops the redundant expression and is easier to read. We can improve it further by reducing the number of operations.</p> <pre><code># good\nhas_operator = ((F.col('originalOperator') != '') | (F.col('currentOperator') != ''))\ndelivery_date_passed = (F.datediff('deliveryDate_actual', 'current_date') &lt; 0)\nhas_registration = (F.col('currentRegistration').rlike('.+'))\nis_delivered = (F.col('prod_status') == 'Delivered')\nis_active = (has_registration | has_operator)\n\nF.when(is_delivered | (delivery_date_passed &amp; is_active), 'In Service')\n</code></pre> <p>Note how the <code>F.when</code> expression is now succinct and readable and the desired behavior is clear to anyone reviewing this code. The reader only needs to visit the individual expressions if they suspect there is an error. It also makes each chunk of logic easy to test if you have unit tests in your code, and want to abstract them as functions.</p>"},{"location":"pyspark/python/#factor-out-common-logic","title":"Factor out common logic","text":"<p><pre><code># bad\ncsv_downloads_today = df.where(\n    (F.col(\"operation\") == \"Download\") &amp; F.col(\"today\") &amp; (F.col(\"file_extension\") == \"csv\")\n)\nexe_downloads_today = df.where(\n    (F.col(\"operation\") == \"Download\") &amp; F.col(\"today\") &amp; (F.col(\"file_extension\") == \"exe\")\n)\n\n\n# good\nDOWNLOADED_TODAY = (F.col(\"operation\") == \"Download\") &amp; F.col(\"today\")\ncsv_downloads_today = df.where(DOWNLOADED_TODAY &amp; (F.col(\"file_extension\") == \"csv\"))\nexe_downloads_today = df.where(DOWNLOADED_TODAY &amp; (F.col(\"file_extension\") == \"exe\"))\n</code></pre> It is okay to reuse these variables even though they include calls to <code>F.col</code>. This prevents repeated code and can make code easier to read.</p>"},{"location":"pyspark/python/#about-select-and-withcolumn","title":"About <code>select()</code> and <code>withColumn()</code>","text":""},{"location":"pyspark/python/#use-select-statements-to-specify-a-schema-contract","title":"Use <code>select</code> statements to specify a schema contract","text":"<p>Doing a select at the beginning of a PySpark transform, or before returning, is considered good practice. This <code>select</code> statement specifies the contract with both the reader and the code about the expected dataframe schema for inputs and outputs. Any select should be seen as a cleaning operation that is preparing the dataframe for consumption by the next step in the transform.</p> <p>Keep select statements as simple as possible. Due to common SQL idioms, allow only one function from <code>spark.sql.function</code> to be used per selected column, plus an optional <code>.alias()</code> to give it a meaningful name. Keep in mind that this should be used sparingly. If there are more than three such uses in the same select, refactor it into a separate function like <code>clean_&lt;dataframe name&gt;()</code> to encapsulate the operation.</p> <p>Expressions involving more than one dataframe, or conditional operations like <code>.when()</code> are discouraged to be used in a select, unless required for performance reasons.</p> <pre><code># bad\naircraft = aircraft.select(\n    'aircraft_id',\n    'aircraft_msn',\n    F.col('aircraft_registration').alias('registration'),\n    'aircraft_type',\n    F.avg('staleness').alias('avg_staleness'),\n    F.col('number_of_economy_seats').cast('long'),\n    F.avg('flight_hours').alias('avg_flight_hours'),\n    'operator_code',\n    F.col('number_of_business_seats').cast('long'),\n)\n</code></pre> <p>Unless order matters to you, try to cluster together operations of the same type.</p> <pre><code># good\naircraft = aircraft.select(\n    'aircraft_id',\n    'aircraft_msn',\n    'aircraft_type',\n    'operator_code',\n    F.col('aircraft_registration').alias('registration'),\n    F.col('number_of_economy_seats').cast('long'),\n    F.col('number_of_business_seats').cast('long'),\n    F.avg('staleness').alias('avg_staleness'),\n    F.avg('flight_hours').alias('avg_flight_hours'),\n)\n</code></pre> <p>The <code>select()</code> statement redefines the schema of a dataframe, so it naturally supports the inclusion or exclusion of columns, old and new, as well as the redefinition of pre-existing ones. By centralising all such operations in a single statement, it becomes much easier to identify the final schema, which makes code more readable. It also makes code more concise.</p>"},{"location":"pyspark/python/#instead-of-using-withcolumn-to-redefine-type-cast-in-the-select","title":"Instead of using <code>withColumn()</code> to redefine type, cast in the select:","text":"<pre><code># bad\ndf.select('comments').withColumn('comments', F.col('comments').cast('double'))\n\n\n# good\ndf.select(F.col('comments').cast('double'))\n</code></pre> <p>But keep it simple: <pre><code># bad\ndf.select(\n    ((F.coalesce(F.unix_timestamp('closed_at'), F.unix_timestamp())\n    - F.unix_timestamp('created_at')) / 86400).alias('days_open')\n)\n\n\n# good\ndf.withColumn(\n    'days_open',\n    (F.coalesce(F.unix_timestamp('closed_at'), F.unix_timestamp()) - F.unix_timestamp('created_at')) / 86400\n)\n</code></pre></p> <p>Avoid including columns in the select statement if they are going to remain unused and choose instead an explicit set of columns - this is a preferred alternative to using <code>.drop()</code> since it guarantees that schema mutations won't cause unexpected columns to bloat your dataframe. However, dropping columns isn't inherintly discouraged in all cases; for instance- it is commonly appropriate to drop columns after joins since it is common for joins to introduce redundant columns. </p> <p>Finally, instead of adding new columns via the select statement, using <code>.withColumn()</code> is recommended instead for single columns. When adding or manipulating tens or hundreds of columns, use a single <code>.select()</code> for performance reasons.</p>"},{"location":"pyspark/python/#joins","title":"Joins","text":""},{"location":"pyspark/python/#be-careful-with-joins","title":"Be careful with joins","text":"<p>If you perform a left join, and the right side has multiple matches for a key, that row will be duplicated as many times as there are matches. This is called a \"join explosion\" and can dramatically bloat the output of your transforms job. Always double check your assumptions to see that the key you are joining on is unique, unless you are expecting the multiplication.</p>"},{"location":"pyspark/python/#explicitly-specifying-how","title":"Explicitly specifying <code>how</code>","text":"<p>Bad joins are the source of many tricky-to-debug issues. There are some things that help like specifying the <code>how</code> (and <code>on</code>)explicitly, even if you are using the default value <code>(inner)</code>:</p> <pre><code># bad\nflights = flights.join(aircraft, 'aircraft_id')\n\n# also bad\nflights = flights.join(aircraft, 'aircraft_id', 'inner')\n\n\n# good\nflights = flights.join(aircraft, on='aircraft_id', how='inner')\n</code></pre>"},{"location":"pyspark/python/#use-left_outer-instead-of-left","title":"Use <code>left_outer</code> instead of <code>left</code>","text":"<p><code>left_outer</code> is more explicit.</p> <pre><code># bad\nflights = flights.join(aircraft, on='aircraft_id', how='left_outer')\n\n\n# good\nflights = flights.join(aircraft, on='aircraft_id', how='left_outer')\n</code></pre>"},{"location":"pyspark/python/#avoid-right_outer-joins","title":"Avoid <code>right_outer</code> joins","text":"<p>If you are about to use a <code>right_outer</code> join, switch the order of your dataframes and use a <code>left_outer</code> join instead. It is more intuitive since the dataframe you are doing the operation on is the one that you are centering your join around.</p> <pre><code># bad\nflights = aircraft.join(flights, on='aircraft_id', how='right_outer')\n\n\n# good\nflights = flights.join(aircraft, on='aircraft_id', how='left_outer')\n</code></pre>"},{"location":"pyspark/python/#avoid-renaming-all-columns-to-avoid-collisions","title":"Avoid renaming all columns to avoid collisions","text":"<p>Avoid renaming all columns to avoid collisions. Instead, give an alias to the whole dataframe, and use that alias to select which columns you want in the end.</p> <pre><code># bad\ncolumns = ['start_time', 'end_time', 'idle_time', 'total_time']\nfor col in columns:\n    flights = flights.withColumnRenamed(col, 'flights_' + col)\n    parking = parking.withColumnRenamed(col, 'parking_' + col)\n\nflights = flights.join(parking, on='flight_code', how='left_outer')\n\nflights = flights.select(\n    F.col('flights_start_time').alias('flight_start_time'),\n    F.col('flights_end_time').alias('flight_end_time'),\n    F.col('parking_total_time').alias('client_parking_total_time')\n)\n\n\n# good\nflights = flights.alias('flights')\nparking = parking.alias('parking')\n\nflights = flights.join(parking, on='flight_code', how='left_outer')\n\nflights = flights.select(\n    F.col('flights.start_time').alias('flight_start_time'),\n    F.col('flights.end_time').alias('flight_end_time'),\n    F.col('parking.total_time').alias('client_parking_total_time')\n)\n</code></pre> <p>In such cases, keep in mind:</p> <ol> <li>It's probably best to drop overlapping columns prior to joining if you don't need both;</li> <li>In case you do need both, it might be best to rename one of them prior to joining;</li> <li>You should always resolve ambiguous columns before outputting a dataset. After the transform is finished running you can no longer distinguish them.</li> </ol>"},{"location":"pyspark/python/#dont-use-dropduplicates-or-distinct-as-a-crutch","title":"Don't use <code>.dropDuplicates()</code> or <code>.distinct()</code> as a crutch","text":"<p>As a last word about joins, don't use <code>.dropDuplicates()</code> or <code>.distinct()</code> as a crutch.  If unexpected duplicate rows are observed, there's almost always an underlying reason for why those duplicate rows appear. Adding <code>.dropDuplicates()</code> only masks this problem and adds overhead to the runtime.</p>"},{"location":"pyspark/python/#window-functions","title":"Window Functions","text":"<p>Always specify an explicit frame when using window functions, using either row frames or range frames. If you do not specify a frame, Spark will generate one, in a way that might not be easy to predict. In particular, the generated frame will change depending on whether the window is ordered (see here). To see how this can be confusing, consider the following example:</p> <pre><code>from pyspark.sql import functions as F, Window as W\ndf = spark.createDataFrame([('a', 1), ('a', 2), ('a', 3), ('a', 4)], ['key', 'num'])\n\n# bad\nw1 = W.partitionBy('key')\nw2 = W.partitionBy('key').orderBy('num')\n\ndf.select('key', F.sum('num').over(w1).alias('sum')).collect()\n# =&gt; [Row(key='a', sum=10), Row(key='a', sum=10), Row(key='a', sum=10), Row(key='a', sum=10)]\n\ndf.select('key', F.sum('num').over(w2).alias('sum')).collect()\n# =&gt; [Row(key='a', sum=1), Row(key='a', sum=3), Row(key='a', sum=6), Row(key='a', sum=10)]\n\ndf.select('key', F.first('num').over(w2).alias('first')).collect()\n# =&gt; [Row(key='a', first=1), Row(key='a', first=1), Row(key='a', first=1), Row(key='a', first=1)]\n\ndf.select('key', F.last('num').over(w2).alias('last')).collect()\n# =&gt; [Row(key='a', last=1), Row(key='a', last=2), Row(key='a', last=3), Row(key='a', last=4)]\n</code></pre> <p>It is much safer to always specify an explicit frame: <pre><code># good\nw3 = W.partitionBy('key').orderBy('num').rowsBetween(W.unboundedPreceding, 0)\nw4 = W.partitionBy('key').orderBy('num').rowsBetween(W.unboundedPreceding, W.unboundedFollowing)\n\ndf.select('key', F.sum('num').over(w3).alias('sum')).collect()\n# =&gt; [Row(key='a', sum=1), Row(key='a', sum=3), Row(key='a', sum=6), Row(key='a', sum=10)]\n\ndf.select('key', F.sum('num').over(w4).alias('sum')).collect()\n# =&gt; [Row(key='a', sum=10), Row(key='a', sum=10), Row(key='a', sum=10), Row(key='a', sum=10)]\n\ndf.select('key', F.first('num').over(w4).alias('first')).collect()\n# =&gt; [Row(key='a', first=1), Row(key='a', first=1), Row(key='a', first=1), Row(key='a', first=1)]\n\ndf.select('key', F.last('num').over(w4).alias('last')).collect()\n# =&gt; [Row(key='a', last=4), Row(key='a', last=4), Row(key='a', last=4), Row(key='a', last=4)]\n</code></pre></p>"},{"location":"pyspark/python/#prefer-use-of-window-functions-to-equivalent-re-joining-operations","title":"Prefer use of window functions to equivalent re-joining operations","text":"<p><pre><code># bad\nresult = downloads.join(\n    downloads.groupby(\"user_id\").agg(F.count(\"*\").alias(\"download_count\")), \"user_id\"\n)\n\n\n# good\nwindow = W.Window.partitionBy(F.col(\"user_id\"))\nresult = downloads.withColumn(\"download_count\", F.count(\"*\").over(window))\n</code></pre> The window function version is usually easier to get right and is usually more concise.</p>"},{"location":"pyspark/python/#avoid-empty-partitionby","title":"Avoid empty <code>partitionBy()</code>","text":"<p>Spark window functions can be applied over all rows, using a global frame. This is accomplished by specifying zero columns in the partition by expression (i.e. <code>W.partitionBy()</code>).</p> <p>Code like this should be avoided, as it forces Spark to combine all data into a single partition, which can be extremely harmful for performance.</p> <p>Prefer to use aggregations whenever possible:</p> <pre><code># bad\nw = W.partitionBy()\ndf = df.select(F.sum('num').over(w).alias('sum'))\n\n# good\ndf = df.agg(F.sum('num').alias('sum'))\n</code></pre>"},{"location":"pyspark/python/#udfs-user-defined-functions","title":"UDFs (user defined functions)","text":"<p>It is highly recommended to avoid UDFs in all situations, as they are dramatically less performant than native PySpark. In most situations, logic that seems to necessitate a UDF can be refactored to use only native PySpark functions.</p>"},{"location":"pyspark/python/#chaining-of-expressions","title":"Chaining of expressions","text":"<p>Keep in mind chaining expressions is a contentious topic. Feedback from team members are important !</p>"},{"location":"pyspark/python/#avoid-chaining-of-expressions-into-multi-line-expressions-with-different-types","title":"Avoid chaining of expressions into multi-line expressions with different types","text":"<p>particularly if they have different behaviours or contexts. For example- mixing column creation or joining with selecting and filtering.</p> <pre><code># bad\ndf = (\n    df\n    .select('a', 'b', 'c', 'key')\n    .filter(F.col('a') == 'truthiness')\n    .withColumn('boverc', F.col('b') / F.col('c'))\n    .join(df2, 'key', how='inner')\n    .join(df3, 'key', how='left_outer')\n    .drop('c')\n)\n\n\n# better (seperating into steps)\n# first: we select and trim down the data that we need\n# second: we create the columns that we need to have\n# third: joining with other dataframes\n\ndf = (\n    df\n    .select('a', 'b', 'c', 'key')\n    .filter(F.col('a') == 'truthiness')\n)\n\ndf = df.withColumn('boverc', F.col('b') / F.col('c'))\n\ndf = (\n    df\n    .join(df2, 'key', how='inner')\n    .join(df3, 'key', how='left_outer')\n    .drop('c')\n)\n</code></pre> <p>Having each group of expressions isolated into its own logical code block improves legibility and makes it easier to find relevant logic. For example, a reader of the code below will probably jump to where they see dataframes being assigned <code>df = df...</code>.</p> <pre><code># bad\ndf = (\n    df\n    .select('foo', 'bar', 'foobar', 'abc')\n    .filter(F.col('abc') == 123)\n    .join(another_table, 'some_field')\n)\n\n\n# better\ndf = (\n    df\n    .select('foo', 'bar', 'foobar', 'abc')\n    .filter(F.col('abc') == 123)\n)\n\ndf = df.join(another_table, on='some_field', how='inner')\n</code></pre>"},{"location":"pyspark/python/#multi-line-expressions","title":"Multi-line expressions","text":"<p>To keep things consistent, please wrap the entire expression into a single parenthesis block, and avoid using <code>\\</code>:</p> <pre><code># bad\ndf = df.filter(F.col('event') == 'executing')\\\n    .filter(F.col('has_tests') == True)\\\n    .drop('has_tests')\n\n\n# good\ndf = (\n  df\n  .filter(F.col('event') == 'executing')\n  .filter(F.col('has_tests') == True)\n  .drop('has_tests')\n)\n</code></pre>"},{"location":"pyspark/python/#when-chaining-several-functions-open-a-cleanly-indentable-block-using-parentheses","title":"When chaining several functions, open a cleanly indentable block using parentheses","text":"<pre><code># bad\nresult = df.groupby(\"user_id\", \"operation\").agg(\n        F.min(\"creation_time\").alias(\"start_time\"),\n        F.max(\"creation_time\").alias(\"end_time\"),\n        F.collect_set(\"client_ip\").alias(\"ips\"),\n)\n\n\n# good\nresult = (\n    df\n    .groupby(\"user_id\", \"operation\")\n    .agg(\n        F.min(\"creation_time\").alias(\"start_time\"),\n        F.max(\"creation_time\").alias(\"end_time\"),\n        F.collect_set(\"client_ip\").alias(\"ips\")\n    )\n)\n</code></pre>"},{"location":"pyspark/python/#try-to-break-the-query-into-reasonably-sized-named-chunks","title":"Try to break the query into reasonably sized named chunks","text":"<p><pre><code># bad (logical chunk not broken out)\ndownloading_user_operations = (\n    logs.join(\n        (logs.where(F.col(\"operation\") == \"Download\").select(\"user_id\").distinct()), \"user_id\"\n    )\n    .groupby(\"user_id\")\n    .agg(\n        F.collect_set(\"operation\").alias(\"operations_used\"),\n        F.count(\"operation\").alias(\"operation_count\"),\n    )\n)\n\n# bad (chunks too small)\ndownload_logs = logs.where(F.col(\"operation\") == \"Download\")\n\ndownloading_users = download_logs.select(\"user_id\").distinct()\n\ndownloading_user_logs = logs.join(downloading_users, \"user_id\")\n\ndownloading_user_operations = downloading_user_logs.groupby(\"user_id\").agg(\n    F.collect_set(\"operation\").alias(\"operations_used\"),\n    F.count(\"operation\").alias(\"operation_count\"),\n)\n\n\n# good\ndownloading_users = logs.where(F.col(\"operation\") == \"Download\").select(\"user_id\").distinct()\n\ndownloading_user_operations = (\n    logs.join(downloading_users, \"user_id\")\n    .groupby(\"user_id\")\n    .agg(\n        F.collect_set(\"operation\").alias(\"operations_used\"),\n        F.count(\"operation\").alias(\"operation_count\"),\n    )\n)\n</code></pre> Choosing when and what to name variables is always a challenge. Resisting the urge to create long PySpark function chains makes the code more readable.</p>"},{"location":"pyspark/python/#foundry-specific","title":"Foundry specific","text":"<p>Please see Foundry tech kit for other development guides.</p>"},{"location":"pyspark/python/#dont-repeat-yourself","title":"Don't repeat yourself","text":"<p>Don't repeat yourself with TO_RENAME and COLS_TO_KEEP and rename only used columns. <pre><code>from transforms.api import transform_df, Input, Output\nfrom dataframe.transformer import rename_columns\n\n# bad\nTO_RENAME = {\n    \"no_das\": \"did_id\",\n    \"no_dossier\": \"depil_id\",\n    \"date_creation\": \"creation_date\",\n    \"date_modification\": \"modification_date\",\n    \"date_suppression\": \"deletion_date\",\n    \"user_maj\": \"update_user\"\n}\nCOLS_TO_KEEP = [\n    \"did_id\",\n    \"depil_id\",\n    \"creation_date\",\n]\n\n@transform_df(\n    Output(\"Users/data/output\"),\n    df=Input(\"Users/data/input\"),\n)\ndef my_compute_function(df):\n\n    df = rename_columns(df, TO_RENAME)\n    df = df.select(*COLS_TO_KEEP)\n    return df\n\n\n# good\nCOLS_TO_KEEP = {\n    \"no_das\": \"did_id\",\n    \"no_dossier\": \"depil_id\",\n    \"date_creation\": \"creation_date\",\n}\n\n@transform_df(\n    Output(\"Users/data/output\"),\n    df=Input(\"Users/data/input\"),\n)\ndef my_compute_function(df):\n\n    df = rename_columns(df, COLS_TO_KEEP)\n    df = df.select(*list(COLS_TO_KEEP.values()))\n    return df\n</code></pre></p>"},{"location":"pyspark/python/#use-check-and-expectations-to-explicit-explain-expected-output-and-input","title":"Use Check and expectations to explicit explain expected Output and Input","text":"<p>please see details in Foundry documentation.</p> <pre><code>from transforms.api import transform_df, Input, Output, Check\nfrom transforms import expectations as E\nfrom dataframe.transformer import get_first_over_window\n\n# bad\n@transform_df(\n    Output(\"Users/data/output\"),\n    df=Input(\"Users/data/input\")\n)\ndef my_compute_function(df):\n    return (get_first_over_window(df, [\"id\"], \"date_maj\", ascOrder=False)\n\n\n# good\n@transform_df(\n    Output(\n        \"Users/data/output\",\n        checks=Check(E.primary_key('id'), 'Primary Key', on_error='FAIL'),\n    ),\n    df=Input(\"Users/data/input\")\n)\ndef my_compute_function(df):\n    return (get_first_over_window(df, [\"id\"], \"date_maj\", ascOrder=False)\n</code></pre>"},{"location":"pyspark/python/#add-require_incrementaltrue-if-you-are-expecting-an-incremental-build","title":"Add <code>require_incremental=True</code> if you are expecting an incremental build","text":"<p>To avoid unexpected snapshot buill create un-wanted data, please add <code>require_incremental=True</code> if you are expecting an incremental build.</p> <pre><code># bad\n\n# BUMPING THE SEMANTIC_VERSION WILL CAUSE ALL HISTORY TO BE LOST\n@incremental(\n    semantic_version=1,\n)\n@transform(\n    out=Output(\"Users/data/output\"),\n    raw=Input(\"Users/data/input\"),\n)\ndef my_compute_function(ctx, out, raw):\n    ...\n\n\n# good\n\n# BUMPING THE SEMANTIC_VERSION WILL CAUSE ALL HISTORY TO BE LOST\n@incremental(\n    semantic_version=1,\n    require_incremental=True  # if it is not incremental it will fails\n)\n@transform(\n    out=Output(\"Users/data/output\"),\n    raw=Input(\"Users/data/input\"),\n)\ndef my_compute_function(ctx, out, raw):\n    ...\n</code></pre>"},{"location":"pyspark/python/#optimization-through-repartitioning","title":"Optimization through (Re)partitioning","text":"<p>please see details in Foundry documentation</p> <p>Partition is the elementary entity on which Spark can execute computation. Spark cannot parallelize computation inside one partition(you need at leaset 2 partitions). The process of tuning number of dataset partitions is called \u201cpartitioning\u201d or \u201cre-partitioning.\u201d  As a general rule, you should aim for a ratio of approximately one partition for every 128MB of your dataset.</p> <pre><code>#bad\n@transform_df(\n    Output(\"Users/data/output\"),\n    df=Input(\"Users/data/input\")\n)\ndef my_compute_function(df):\n    return df    # by default it is df.repartition(4), this might not be optimized\n\n\n\n#good\n@transform_df(\n    Output(\"Users/data/output\"),\n    df=Input(\"Users/data/input\")\n)\ndef my_compute_function(df):\n    # calculate number of partitions following the rule mentioned above, sometimes you might want call df.coalesce\n    return df.repartition(calculated_number)   \n</code></pre>"},{"location":"pyspark/python/#other-considerations-and-recommendations","title":"Other Considerations and Recommendations","text":"<ol> <li>Be wary of functions that grow too large. As a general rule, a file     should not be over 250 lines, and a function should not be over 70 lines.</li> <li>Try to keep your code in logical blocks. For example, if you have     multiple lines referencing the same things, try to keep them     together. Separating them reduces context and readability.</li> <li>Test your code! If you can run the local tests, do so and make     sure that your new code is covered by the tests. If you can't run     the local tests, build the datasets on your branch and manually     verify that the data looks as expected.</li> <li>Avoid <code>.otherwise(value)</code> as a general fallback. If you are mapping     a list of keys to a list of values and a number of unknown keys appear,     using <code>otherwise</code> will mask all of these into one value.</li> <li>Do not keep commented out code checked in the repository. This applies     to single line of codes, functions, classes or modules. Rely on git     and its capabilities of branching or looking at history instead.</li> <li>When encountering a large single transformation composed of integrating multiple different source tables, split it into the natural sub-steps and extract the logic to functions. This allows for easier higher level readability and allows for code re-usability and consistency between transforms.</li> <li>Try to be as explicit and descriptive as possible when naming functions     or variables. Strive to capture what the function is actually doing     as opposed to naming it based the objects used inside of it.</li> <li>Avoid using literal strings or integers in filtering conditions, new     values of columns etc. Instead, to capture their meaning, extract them into variables, constants,     dicts or classes as suitable. This makes the     code more readable and enforces consistency across the repository.</li> </ol>"},{"location":"pyspark/python/#contributing","title":"Contributing","text":"<p>One of the main purposes of this document is to encourage consistency. Some choices made here are arbitrary, but we hope they will lead to more readable code. Other choices may prove wrong with more time and experience. Suggestions for changes to the guide or additions to it are welcome. Please feel free to create an issue or pull request or chat directly in teams to start a discussion.</p>"},{"location":"pyspark/spark/","title":"asdasd","text":"<p>asdasd a sd as da sd asd Test </p>"},{"location":"pyspark/examples/first/","title":"First example","text":"<p>asd asd</p>"},{"location":"pyspark/examples/first/#1-","title":"1- ...","text":""},{"location":"pyspark/examples/first/#second-example","title":"Second example","text":"<p>asda</p>"}]}